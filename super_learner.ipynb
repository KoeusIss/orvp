{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a981b331-b924-4204-8d5f-f1077b205483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import seaborn as sns\n",
    "\n",
    "# Utiilities\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Constant\n",
    "BASE_PATH = \"DS/optiver-realized-volatility-prediction/\"\n",
    "TEST_FILE = BASE_PATH + \"test.csv\"\n",
    "TRAIN_FILE = BASE_PATH + \"train.csv\"\n",
    "\n",
    "N_FOLDS = 10\n",
    "N_REPEAT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439df84b-2926-4b4d-866b-67d99a1d3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial utilities functions\n",
    "\n",
    "def wap(df, index):\n",
    "    \"\"\"\n",
    "    Compute the Waighted Avergae Price\n",
    "    \"\"\"\n",
    "    bid_price = df[f\"bid_price{index}\"]\n",
    "    bid_size = df[f\"bid_size{index}\"]\n",
    "    ask_price = df[f\"ask_price{index}\"]\n",
    "    ask_size = df[f\"ask_size{index}\"]\n",
    "    return bid_price * ask_size + ask_price * bid_size / (bid_size + ask_size)\n",
    "\n",
    "def log_return(series):\n",
    "    \"\"\"\n",
    "    Compute the log_return of WAP\n",
    "    \"\"\"\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    \"\"\"\n",
    "    Compute the realized volatility\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def total_volume(df):\n",
    "    \"\"\"\n",
    "    Compute the total volume between asks and bids\n",
    "    \"\"\"\n",
    "    ask_size_1 = df[\"ask_size1\"]\n",
    "    ask_size_2 = df[\"ask_size2\"]\n",
    "    bid_size_1 = df[\"bid_size1\"]\n",
    "    bid_size_2 = df[\"bid_size2\"]\n",
    "    return ask_size_1 + ask_size_2 + bid_size_1 + bid_size_2\n",
    "\n",
    "def imbalance_volume(df):\n",
    "    \"\"\"\n",
    "    Computes the imbalance volume\n",
    "    \"\"\"\n",
    "    ask_size_1 = df[\"ask_size1\"]\n",
    "    ask_size_2 = df[\"ask_size2\"]\n",
    "    bid_size_1 = df[\"bid_size1\"]\n",
    "    bid_size_2 = df[\"bid_size2\"]\n",
    "    return np.abs(ask_size_1 + ask_size_2 - bid_size_1 - bid_size_2)\n",
    "\n",
    "def spread_size(df, type):\n",
    "    \"\"\"\n",
    "    Compute the spread\n",
    "    \"\"\"\n",
    "    size_1 = df[f\"{type}_size1\"]\n",
    "    size_2 = df[f\"{type}_size2\"]\n",
    "    return size_1 - size_2\n",
    "\n",
    "def spread_price(df, type):\n",
    "    \"\"\"\n",
    "    Compute the spread\n",
    "    \"\"\"\n",
    "    price_1 = df[f\"{type}_price1\"]\n",
    "    price_2 = df[f\"{type}_price2\"]\n",
    "    return price_1 - price_2\n",
    "\n",
    "def price_spread(df, index):\n",
    "    \"\"\"\n",
    "    Compute the price spred\n",
    "    \"\"\"\n",
    "    ask_price = df[f\"ask_price{index}\"]\n",
    "    bid_price = df[f\"bid_price{index}\"]\n",
    "    return (ask_price - bid_price) / (ask_price + bid_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9f6c6e-ccb9-4c5b-a132-71d736cad81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_book_features(df, window_size=100):\n",
    "    \"\"\"\n",
    "    Set book features // Windowing rows on columns\n",
    "    \"\"\"\n",
    "    # First level aggregation\n",
    "    df[\"wap_1\"]                = wap(df, 1)\n",
    "    df[\"wap_2\"]                = wap(df, 2)\n",
    "    df[\"log_return_1\"]         = df.groupby(\"time_id\")[\"wap_1\"].apply(log_return)\n",
    "    df[\"log_return_2\"]         = df.groupby(\"time_id\")[\"wap_2\"].apply(log_return)\n",
    "    df[\"bid_size_spread\"]      = spread_size(df, \"bid\")\n",
    "    df[\"bid_price_spread\"]     = spread_price(df, \"bid\")\n",
    "    df[\"ask_size_spread\"]      = spread_size(df, \"ask\")\n",
    "    df[\"ask_price_spread\"]     = spread_price(df, \"ask\")\n",
    "    df[\"price_spread_1\"]       = price_spread(df, 1)\n",
    "    df[\"price_spread_2\"]       = price_spread(df, 2)\n",
    "    df['wap_balance']          = np.abs(df['wap_1'] - df['wap_2'])\n",
    "    df[\"bid_ask_price_spread\"] = np.abs(df[\"ask_price_spread\"] - df[\"bid_price_spread\"])\n",
    "    df[\"bid_ask_size_spread\"]  = np.abs(df[\"ask_size_spread\"] - df[\"bid_size_spread\"])\n",
    "    df[\"total_volume\"]         = total_volume(df)\n",
    "    df[\"imbalance_volume\"]     = imbalance_volume(df)\n",
    "    \n",
    "    # Second level aggregation\n",
    "    aggregation_dict = {\n",
    "        \"log_return_1\": [realized_volatility, np.mean],\n",
    "        \"log_return_2\": [realized_volatility, np.mean],\n",
    "        \"wap_1\": [np.mean, np.std],\n",
    "        \"wap_2\": [np.mean, np.std],\n",
    "        \"bid_price1\": [np.mean, np.max],\n",
    "        \"bid_price2\": [np.mean, np.max],\n",
    "        \"ask_price1\": [np.mean, np.min],\n",
    "        \"ask_price2\": [np.mean, np.min],\n",
    "        \"bid_size1\": [np.mean, np.sum],\n",
    "        \"bid_size2\": [np.mean, np.sum],\n",
    "        \"ask_size1\": [np.mean, np.sum],\n",
    "        \"ask_size2\": [np.mean, np.sum],\n",
    "        \"bid_size_spread\": [np.mean, np.std],\n",
    "        \"bid_price_spread\": [np.mean, np.std],\n",
    "        \"ask_size_spread\": [np.mean, np.std],\n",
    "        \"ask_price_spread\": [np.mean, np.std],\n",
    "        \"price_spread_1\": [np.mean, np.std],\n",
    "        \"price_spread_2\": [np.mean, np.std],\n",
    "        \"wap_balance\": [np.mean, np.std],\n",
    "        \"bid_ask_price_spread\": [np.mean, np.std],\n",
    "        \"bid_ask_size_spread\": [np.mean, np.std],\n",
    "        \"total_volume\": [np.mean, np.std],\n",
    "        \"imbalance_volume\": [np.mean, np.std]\n",
    "    }\n",
    "    features_lst = []\n",
    "    for second in range(0, 600, window_size):\n",
    "        df_feature = df[df['seconds_in_bucket'] >= second].groupby(\"time_id\").agg(aggregation_dict)\n",
    "        df_feature.columns = ['_'.join(col) + f\"_s{600 - second}\" for col in df_feature.columns]\n",
    "        features_lst.append(df_feature)\n",
    "    return pd.concat(features_lst, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84930c19-492c-44da-9749-7ed2621cdad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trade_features(df, window_size=100):\n",
    "    \"\"\"\n",
    "    Set trade features // Windowing rows on columns\n",
    "    \"\"\"\n",
    "    # First level aggregation\n",
    "    df[\"log_return\"]   = df.groupby(\"time_id\")[\"price\"].apply(log_return)\n",
    "    df['amount']       = df['price'] * df['size']\n",
    "    \n",
    "    # Second level aggregation\n",
    "    aggregation_dict = {\n",
    "        \"log_return\": [realized_volatility, np.mean],\n",
    "        \"size\": [np.mean, np.sum],\n",
    "        \"amount\": [np.mean, np.sum],\n",
    "        \"order_count\": [np.mean, np.sum],\n",
    "    }\n",
    "    features_lst = []\n",
    "    for second in range(0, 600, window_size):\n",
    "        df_feature = df[df['seconds_in_bucket'] >= second].groupby(\"time_id\").agg(aggregation_dict)\n",
    "        df_feature.columns = ['_'.join(col) + f\"_s{600 - second}\" for col in df_feature.columns]\n",
    "        features_lst.append(df_feature)\n",
    "    return pd.concat(features_lst, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98745ed-f238-4925-9489-1bc223f52121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ids_list, mode, window_size=60):\n",
    "    \"\"\"\n",
    "    Preprocess dataset\n",
    "    \"\"\"\n",
    "    def create_stock_df(stock_id):\n",
    "        book = pd.read_parquet(f\"{BASE_PATH}book_{mode}.parquet/stock_id={stock_id}\")\n",
    "        trade = pd.read_parquet(f\"{BASE_PATH}trade_{mode}.parquet/stock_id={stock_id}\")\n",
    "\n",
    "        book_features = set_book_features(book, window_size)\n",
    "        trade_features = set_trade_features(trade, window_size)\n",
    "        \n",
    "        features = book_features.join(trade_features, how='outer')\n",
    "        features = features.reset_index()\n",
    "        features[\"stock_id\"] = stock_id\n",
    "        return features\n",
    "      \n",
    "    df_list = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(create_stock_df)(_id) for _id in ids_list\n",
    "    )\n",
    "    return pd.concat(df_list, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e80e751-2eb4-428b-86ef-e314de58d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train file\n",
    "train = pd.read_csv(TRAIN_FILE)\n",
    "train_ids = train.stock_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df47a83-ff73-4872-b88c-05d66c020704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 10.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data + FE\n",
    "train_df = preprocess(train_ids, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab48251-8594-4a01-9167-0278ed19a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and select\n",
    "df_train = train.merge(train_df, on=[\"stock_id\", \"time_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3f2c4aa-ee51-4b6e-aac1-05dd7f16873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward filling the missing\n",
    "df_train = df_train.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "433bf1e6-b179-40c3-a191-c0b23ae44fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(\"target\", axis=1)\n",
    "y = df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cf914ed-f1e6-4b71-acfd-f038786f7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f828a868-7f8c-4893-8c77-bde61b066221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model library\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "917a8889-1b2e-4083-91ed-bacb08f821f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get OOF prediction\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "    meta_X, meta_y = list(), list()\n",
    "    kfold = KFold(n_splits=N_FOLDS, shuffle=True)\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "        fold_yhats = list()\n",
    "        \n",
    "        train_X, test_X = X[train_ix], X[test_ix]\n",
    "        train_y, test_y = y[train_ix], y[test_ix]\n",
    "        meta_y.extend(test_y)\n",
    "\n",
    "        for model in models:\n",
    "            model.fit(train_X, train_y)\n",
    "            yhat = model.predict(test_X)\n",
    "            fold_yhats.append(yhat.reshape(len(yhat),1))\n",
    "\n",
    "        meta_X.append(np.hstack(fold_yhats))\n",
    "    return np.vstack(meta_X), np.asarray(meta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbd9640a-23cd-4ed0-bc83-0d7f2d7e70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_base_models(X, y, models):\n",
    "    for model in models:\n",
    "        model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "449dd0b8-17b6-43f0-9d5a-853264161b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_meta_model(X, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72190767-9483-4feb-ae58-3a6423e855c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Root Mean Squate Percentage Error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33945e14-6771-4c40-877d-9f6112c006ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X, y, models):\n",
    "    for model in models:\n",
    "        yhat = model.predict(X)\n",
    "        _rmspe = rmspe(y, yhat)\n",
    "        print('%s: RMSPE %.5f' % (model.__class__.__name__, rmspe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "773d11c8-85c4-43a8-b58f-fe1b2571d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_learner_predictions(X, models, meta_model):\n",
    "    meta_X = list()\n",
    "    for model in models:\n",
    "        yhat = model.predict(X)\n",
    "        meta_X.append(yhat.reshape(len(yhat),1))\n",
    "    meta_X = np.hstack(meta_X)\n",
    "\n",
    "    return meta_model.predict(meta_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3d0db-a151-47e2-9750-d48df6c38001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super\"Learner\" Call\n",
    "fe, fe_test, label, label_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "meta_X, meta_y = get_out_of_fold_predictions(fe, label, models)\n",
    "fit_base_models(fe, label, models)\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "evaluate_models(fe_test, label_test, models)\n",
    "yhat = super_learner_predictions(fe_test, models, meta_model)\n",
    "print('Super Learner: RMSPE %.5f' % rmspe(label_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7c778-dc06-46c5-8bc4-2ee090295395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
